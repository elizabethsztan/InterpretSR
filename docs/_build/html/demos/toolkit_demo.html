

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>SymTorch Toolkit: Pruning &mdash; SymTorch 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=8d563738"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Getting Started" href="getting_started_demo.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            SymTorch
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_reference.html">API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Demos:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="getting_started_demo.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">SymTorch Toolkit: Pruning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#wrapping-a-pytorch-model">Wrapping a PyTorch model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training-our-model-and-dynamically-reducing-dimensionality">Training our model and dynamically reducing dimensionality</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interpret-the-mlp">Interpret the MLP</a></li>
<li class="toctree-l2"><a class="reference internal" href="#switch-to-using-the-equation-instead-in-the-forwards-pass">Switch to Using the Equation Instead in the Forwards Pass</a></li>
<li class="toctree-l2"><a class="reference internal" href="#switch-to-using-the-mlp-in-the-forwards-pass">Switch to Using the MLP in the Forwards Pass</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SymTorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">SymTorch Toolkit: Pruning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/demos/toolkit_demo.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="symtorch-toolkit-pruning">
<h1>SymTorch Toolkit: Pruning<a class="headerlink" href="#symtorch-toolkit-pruning" title="Link to this heading"></a></h1>
<p>For interpretability purposes, it is good to reduce the dimensionality of deep learning models. High-dimensional representations often entangle multiple features, making it difficult to extract clear, human-understandable relationships. By encouraging a sparse representation, we encourage the network to compress information into a smaller set of meaningful components. This may also make symbolic regression possible on these models.</p>
<p>The SymTorch pruning class allows you to dynamically reduce the output dimensionality of MLPs by zero-masking the unimportant dimensions.</p>
<p><strong>Important dimensions</strong>: The dimensions that the model uses the most in predicting the output. These would vary most with differences in the input. Hence we choose the important dimensions as the ones with the highest standard deviation across the datapoints.</p>
<p>We pass some input data through the model (usually a subset of the validation set) and analyse the outputs of the MLP. We choose the output dimensions that have the highest standard deviation across the datapoints, as shown below.</p>
<img src="../_static/choosing_important_dims.png" width="450" height="300"><section id="wrapping-a-pytorch-model">
<h2>Wrapping a PyTorch model<a class="headerlink" href="#wrapping-a-pytorch-model" title="Link to this heading"></a></h2>
<p>Create a simple PyTorch model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simple MLP.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SimpleModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Model with MLP f_net and linear g_net.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">output_dim_f</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">f_net</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim_f</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="c1"># g is linear - only learns to combine the 2 pruned outputs from f</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">g_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">output_dim_f</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>  <span class="c1"># Will use first 2 dims of f after pruning</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">f_net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">g_net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>Train the model on some data. We have a composite function <span class="math notranslate nohighlight">\(y=g(f(\mathbf{x}))\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp; f_0 = x_0^2\\
&amp; f_1 = \sin{x_4}
\end{aligned}
\end{split}\]</div>
<p>and <span class="math notranslate nohighlight">\(g\)</span> is just a linear transformation of <span class="math notranslate nohighlight">\(f_0\)</span> and <span class="math notranslate nohighlight">\(f_1\)</span>
$<span class="math notranslate nohighlight">\(
g(\mathbf{f}) = 2.5f_0 -1.3f_1
\)</span>$</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Make the dataset </span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10_000</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span>

<span class="k">def</span><span class="w"> </span><span class="nf">f_func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">f0</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> 
    <span class="n">f1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">4</span><span class="p">])</span>  
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">f0</span><span class="p">,</span> <span class="n">f1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">g_func</span><span class="p">(</span><span class="n">f_output</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3</span>  
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">f_output</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">f_output</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Generate ground truth data</span>
<span class="n">f_true</span> <span class="o">=</span> <span class="n">f_func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">g_func</span><span class="p">(</span><span class="n">f_true</span><span class="p">)</span>

<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.05</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="n">noise</span> 
</pre></div>
</div>
</div>
</div>
<p>We need to set up the pruning model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">symtorch.toolkit</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pruning_MLP</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create model with pruning for f, linear g_net</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_dim_f</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">f_net</span> <span class="o">=</span> <span class="n">Pruning_MLP</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">f_net</span><span class="p">,</span>
                      <span class="n">initial_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="c1"># Initial dimensionality of the MLP</span>
                      <span class="n">target_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># Target dimensionality - final output dim after pruning</span>
                      <span class="n">mlp_name</span><span class="o">=</span><span class="s2">&quot;f_net&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-our-model-and-dynamically-reducing-dimensionality">
<h2>Training our model and dynamically reducing dimensionality<a class="headerlink" href="#training-our-model-and-dynamically-reducing-dimensionality" title="Link to this heading"></a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set up the pruning schedule</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">model</span><span class="o">.</span><span class="n">f_net</span><span class="o">.</span><span class="n">set_schedule</span><span class="p">(</span><span class="n">total_epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> 
                     <span class="n">end_epoch_frac</span><span class="o">=</span><span class="mf">0.7</span> <span class="c1"># End pruning after 70% of epochs</span>
                     <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set up training</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Train model with MLP f (with pruning) and linear g_net.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        model: PyTorch model to train</span>
<span class="sd">        dataloader: DataLoader for training data</span>
<span class="sd">        X_val, y_val: Validation data for pruning</span>
<span class="sd">        opt: Optimizer</span>
<span class="sd">        criterion: Loss function</span>
<span class="sd">        epochs: Number of training epochs</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        tuple: (trained_model, loss_tracker, active_dims_tracker)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">loss_tracker</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">active_dims_tracker</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        
        <span class="k">for</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="c1"># Forward pass</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">)</span>
            <span class="c1"># Backward pass</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            
            <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="n">loss_tracker</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">)</span>
        <span class="n">active_dims_tracker</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">f_net</span><span class="o">.</span><span class="n">pruning_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">model</span><span class="o">.</span><span class="n">f_net</span><span class="o">.</span><span class="n">prune</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">X_val</span><span class="p">)</span> <span class="c1"># Pass in the validation set (or a subset of) to the model</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
            <span class="n">active_dims</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">f_net</span><span class="o">.</span><span class="n">pruning_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s1">], Avg Loss: </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">:</span><span class="s1">.6f</span><span class="si">}</span><span class="s1">, Active dims: </span><span class="si">{</span><span class="n">active_dims</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_tracker</span><span class="p">,</span> <span class="n">active_dims_tracker</span>

<span class="c1"># Set up training</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="c1"># Split data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">290402</span><span class="p">)</span>

<span class="c1"># Set up dataset - only x as input now</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train the model and save the weights</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting training...&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">active_dims</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X_val</span><span class="p">),</span> <span class="n">opt</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training completed!&quot;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;model_weights.pth&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Starting training...
Epoch [10/100], Avg Loss: 0.014917, Active dims: 30
Epoch [20/100], Avg Loss: 0.011608, Active dims: 26
Epoch [30/100], Avg Loss: 0.009897, Active dims: 20
Epoch [40/100], Avg Loss: 0.009762, Active dims: 14
Epoch [50/100], Avg Loss: 0.009537, Active dims: 8
Epoch [60/100], Avg Loss: 0.008782, Active dims: 3
Epoch [70/100], Avg Loss: 0.008280, Active dims: 2
Epoch [80/100], Avg Loss: 0.008194, Active dims: 2
Epoch [90/100], Avg Loss: 0.007955, Active dims: 2
Epoch [100/100], Avg Loss: 0.007329, Active dims: 2
Training completed!
</pre></div>
</div>
</div>
</div>
<p>Let’s see how the number of active dimensions decrease as training progesses.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">active_dims</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epochs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Number of active dimensions for the f MLP output&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/674c17dbf46232ffd55998e78112cd0e9840d2ade55228d9462d841481fb67bc.png" src="../_images/674c17dbf46232ffd55998e78112cd0e9840d2ade55228d9462d841481fb67bc.png" />
</div>
</div>
<p>You can pass a <code class="docutils literal notranslate"><span class="pre">decay_rate</span></code> parameter into the <code class="docutils literal notranslate"><span class="pre">.set_schedule</span></code> method of a <code class="docutils literal notranslate"><span class="pre">Pruning_MLP</span></code>. The default is a cosine decay (as shown above). The other options are <code class="docutils literal notranslate"><span class="pre">exp</span></code> and <code class="docutils literal notranslate"><span class="pre">linear</span></code>.</p>
<img src="../_static/pruning_decay_schedules.png">
In the above image, the pruning finishes at epoch 75 and we prune 100 dimensions to 2 dimensions.</section>
<section id="interpret-the-mlp">
<h2>Interpret the MLP<a class="headerlink" href="#interpret-the-mlp" title="Link to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">.interpret</span></code> function only takes into account the active (non-masked) dimensions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Running symbolic regression on pruned f...&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">f_net</span><span class="o">.</span><span class="n">interpret</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> 
                       <span class="n">niterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                       <span class="n">verbosity</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                       <span class="n">complexity_of_operators</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;sin&quot;</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;exp&quot;</span><span class="p">:</span><span class="mi">3</span><span class="p">},</span> 
                       <span class="n">constraints</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;sin&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;exp&quot;</span><span class="p">:</span><span class="mi">3</span><span class="p">},</span>
                       <span class="n">complexity_of_constants</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                       <span class="n">parsimony</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running symbolic regression on pruned f...
🛠️ Running SR on active dimension 11 (1/2)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/liz/PhD/SymTorch_project/symtorch_venv/lib/python3.11/site-packages/pysr/sr.py:2811: UserWarning: Note: it looks like you are running in Jupyter. The progress bar will be turned off.
  warnings.warn(
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>💡Best equation for active dimension 11: ((sin(x4) * -4.0603356) + -0.4592915) + ((x0 * 7.924281) * x0)
🛠️ Running SR on active dimension 30 (2/2)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/liz/PhD/SymTorch_project/symtorch_venv/lib/python3.11/site-packages/pysr/sr.py:2811: UserWarning: Note: it looks like you are running in Jupyter. The progress bar will be turned off.
  warnings.warn(
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>💡Best equation for active dimension 30: ((sin(x4) * -4.253878) + -0.47904623) + ((x0 * 8.191602) * x0)
❤️ SR on f_net active dimensions complete.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{11: PySRRegressor.equations_ = [
 	    pick     score                                           equation  \
 	0         0.000000                                                 x0   
 	1         0.060504                                            x0 + x0   
 	2         0.012885                                     x0 * 2.4303756   
 	3         0.099476                                     (x0 + x0) * x0   
 	4         0.251562                         inv(inv(x0) + -0.83665174)   
 	5         0.893290                      (x0 + -0.46222934) * 7.921155   
 	6         0.223472               (x4 * -5.1028423) + (x0 * 6.3036585)   
 	7         1.203393         (x0 * (x0 * 7.5073485)) + (x4 * -4.141273)   
 	8         0.202045  (x4 * -4.281107) + (((x4 + 7.1182756) * x0) * x0)   
 	9         1.395236  ((x0 * (x0 * 7.9227934)) + -0.5894524) + (x4 *...   
 	10  &gt;&gt;&gt;&gt;  0.192849  ((sin(x4) * -4.0603356) + -0.4592915) + ((x0 *...   
 	11        0.002112  ((((x0 * 7.835847) + 0.09281631) * x0) + (sin(...   
 	12        0.002932  (x0 * (x0 * ((x4 * 0.10984373) + 7.8695107))) ...   
 	13        0.006167  ((x0 * (((x0 + x0) * -0.13601798) + 8.17911)) ...   
 	14        0.001548  (sin(x4) * -4.0556655) + ((((((sin(x0) + -0.57...   
 	
 	        loss  complexity  
 	0   5.428722           1  
 	1   4.809997           3  
 	2   4.748418           4  
 	3   4.298799           5  
 	4   3.342684           6  
 	5   1.368184           7  
 	6   0.875062           9  
 	7   0.078847          11  
 	8   0.052637          13  
 	9   0.013042          14  
 	10  0.007313          17  
 	11  0.007267          20  
 	12  0.007224          22  
 	13  0.007136          24  
 	14  0.007070          30  
 ],
 30: PySRRegressor.equations_ = [
 	    pick     score                                           equation  \
 	0         0.000000                                                 x0   
 	1         0.058238                                            x0 + x0   
 	2         0.014227                                     x0 * 2.4712734   
 	3         0.091449                                     (x0 + x0) * x0   
 	4         0.250775                         inv(inv(x0) + -0.84148186)   
 	5         0.915017                       (x0 * 8.207496) + -3.8249905   
 	6         0.234240               ((x4 * -0.81828034) + x0) * 6.522621   
 	7         1.182880        (x4 * -4.3366895) + ((x0 * x0) * 7.7566714)   
 	8         0.302618   ((x0 * x0) * 7.87612) + ((x4 + -5.1502504) * x4)   
 	9         1.171825  ((x4 * -3.6367) + -0.6148395) + ((x0 * x0) * 8...   
 	10  &gt;&gt;&gt;&gt;  0.162081  ((sin(x4) * -4.253878) + -0.47904623) + ((x0 *...   
 	11        0.033237  ((sin(x4) * -4.2536726) + (x0 * ((x0 * 7.81192...   
 	12        0.015804  (sin(x4) * -4.461439) + ((x0 * ((x0 * 8.020732...   
 	
 	        loss  complexity  
 	0   5.875321           1  
 	1   5.229334           3  
 	2   5.155465           4  
 	3   4.704918           5  
 	4   3.661356           6  
 	5   1.466409           7  
 	6   0.917903           9  
 	7   0.086171          11  
 	8   0.047045          13  
 	9   0.014574          14  
 	10  0.008962          17  
 	11  0.008112          20  
 	12  0.007859          22  
 ]}
</pre></div>
</div>
</div>
</div>
<p>You can see that the outputs of the <code class="docutils literal notranslate"><span class="pre">f_net</span></code> NN are linear combinations of the f function.</p>
<p>We can even perform SR on the <code class="docutils literal notranslate"><span class="pre">g_net</span></code> to show that this layer is just a linear transformation of the inputs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">symtorch.mlp_sr</span><span class="w"> </span><span class="kn">import</span> <span class="n">MLP_SR</span>

<span class="n">model</span><span class="o">.</span><span class="n">g_net</span> <span class="o">=</span> <span class="n">MLP_SR</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">g_net</span><span class="p">,</span> <span class="n">mlp_name</span><span class="o">=</span><span class="s1">&#39;g_net&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">g_net</span><span class="o">.</span><span class="n">interpret</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> 
                     <span class="n">parent_model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                     <span class="n">verbosity</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                     <span class="n">niterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                     <span class="n">complexity_of_operators</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;sin&quot;</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;exp&quot;</span><span class="p">:</span><span class="mi">3</span><span class="p">},</span> 
                     <span class="n">constraints</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;sin&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;exp&quot;</span><span class="p">:</span><span class="mi">3</span><span class="p">},</span>
                     <span class="n">complexity_of_constants</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                     <span class="n">parsimony</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>🛠️ Running SR on output dimension 0 of 0
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/liz/PhD/SymTorch_project/symtorch_venv/lib/python3.11/site-packages/pysr/sr.py:2811: UserWarning: Note: it looks like you are running in Jupyter. The progress bar will be turned off.
  warnings.warn(
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>💡Best equation for output 0 found to be (x30 * 0.1593529) + ((x11 * 0.14723374) + 0.13027407).
❤️ SR on g_net complete.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{0: PySRRegressor.equations_ = [
 	   pick     score                                           equation  \
 	0        0.000000                                                x15   
 	1        0.069353                                         0.21549018   
 	2        1.863556                                   x11 * 0.31768548   
 	3        0.122595                          (x30 * -0.65978885) + x11   
 	4        5.711863                      0.13471259 + (x30 * 0.301455)   
 	5        3.172880             ((x30 + x11) * 0.1534004) + 0.13008839   
 	6  &gt;&gt;&gt;&gt;  5.856868  (x30 * 0.1593529) + ((x11 * 0.14723374) + 0.13...   
 	
 	           loss  complexity  
 	0  6.930116e-01           1  
 	1  6.465779e-01           2  
 	2  1.555812e-02           4  
 	3  1.217511e-02           6  
 	4  4.025709e-05           7  
 	5  7.061770e-08           9  
 	6  1.652331e-15          12  
 ]}
</pre></div>
</div>
</div>
</div>
<p>The variables used in this NN are just the active dimensions of the <code class="docutils literal notranslate"><span class="pre">f_net</span></code> NN.</p>
</section>
<section id="switch-to-using-the-equation-instead-in-the-forwards-pass">
<h2>Switch to Using the Equation Instead in the Forwards Pass<a class="headerlink" href="#switch-to-using-the-equation-instead-in-the-forwards-pass" title="Link to this heading"></a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">f_net</span><span class="o">.</span><span class="n">switch_to_equation</span><span class="p">()</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>✅ Successfully switched f_net to symbolic equations for 2 active dimensions:
   Dimension 11: ((sin(x4) * -4.0603356) + -0.4592915) + ((x0 * 7.924281) * x0)
   Variables: [&#39;x0&#39;, &#39;x4&#39;]
   Dimension 30: ((sin(x4) * -4.253878) + -0.47904623) + ((x0 * 8.191602) * x0)
   Variables: [&#39;x0&#39;, &#39;x4&#39;]
🎯 Active dimensions [11, 30] now using symbolic equations.
🔒 Inactive dimensions will output zeros.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">g_net</span><span class="o">.</span><span class="n">switch_to_equation</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>✅ Successfully switched g_net to symbolic equations for all 1 dimensions:
   Dimension 0: (x30 * 0.1593529) + ((x11 * 0.14723374) + 0.13027407)
   Variables: [&#39;x11&#39;, &#39;x30&#39;]
🎯 All 1 output dimensions now using symbolic equations.
</pre></div>
</div>
</div>
</div>
<p>Now when running the forwards pass through the model, it uses the symbolic equation instead of the MLP.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">interpretable_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">interpretable_outputs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.0448],
        [ 1.0667],
        [ 0.9758],
        ...,
        [ 1.0348],
        [ 1.1453],
        [-0.4258]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="switch-to-using-the-mlp-in-the-forwards-pass">
<h2>Switch to Using the MLP in the Forwards Pass<a class="headerlink" href="#switch-to-using-the-mlp-in-the-forwards-pass" title="Link to this heading"></a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">f_net</span><span class="o">.</span><span class="n">switch_to_mlp</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">g_net</span><span class="o">.</span><span class="n">switch_to_mlp</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>✅ Switched f_net back to MLP
✅ Switched g_net back to MLP
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">model_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">model_outputs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.0496],
        [ 1.0791],
        [ 0.9897],
        ...,
        [ 1.0486],
        [ 1.1706],
        [-0.4119]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Clean up </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>
<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;SR_output&#39;</span><span class="p">):</span>
    <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="s1">&#39;SR_output&#39;</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">&#39;model_weights.pth&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="getting_started_demo.html" class="btn btn-neutral float-left" title="Getting Started" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>